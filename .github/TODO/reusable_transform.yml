on:
  workflow_call:
    inputs:
      ENV_FILE:
        required: true
        type: string
      INPUT_SCHEMA_FAA:
        required: true
        type: string
      INPUT_SCHEMA_GITHUB:
        required: true
        type: string
      INPUT_SCHEMA_EXCHANGERATEHOST:
        required: true
        type: string
      INPUT_SCHEMA_ECOMMERCE_DEMO:
        required: true
        type: string
      INPUT_SCHEMA_DATA_SCIENCE:
        required: true
        type: string

jobs:
  reusable_transform:
    environment: ${{ inputs.ENVIRONMENT }}
    env:
      GIT_DEPTH: "0"
    runs-on: ubuntu-latest
    container: ghcr.io/$GITHUB_REPOSITORY/${{ env.DBT_CUSTOM_IMAGE }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Set Environment Variables
        uses: ./.github/actions/setvars
        with:
          varFilePath: "./.github/variables/elta_shared.env ${{ inputs.ENV_FILE }}"

      - name: Setup Environment
        env:
          GOODDATA_PROFILES_FILE: "${{ secrets.GOODDATA_PROFILES_FILE }}"
        run: |
          mkdir -p ~/.gooddata
          cp $GOODDATA_PROFILES_FILE ~/.gooddata/profiles.yaml
          cd $SRC_DATA_PIPELINE
          # dbt packages are installed during build of docker image to workdir
          ln -s ${IMAGES_WORKDIR}/dbt_packages dbt_packages
          echo $GOODDATA_PROFILES_FILE | base64 --decode > ~/.gooddata/profiles.yaml

      - name: Run Transform
        timeout-minutes: 15
        env:
          FR_ARG: ${{ env.FULL_REFRESH == 'true' && '--full-refresh' || '' }}
          # dbt cloud insist on env variables must contain DBT_ prefix. We have to duplicate them here.
          # dbt profiles.yml file in this repo relies on DBT_ prefix.
          # It means that even jobs not running against dbt cloud rely on DBT_ prefix.
          # More variables are duplicated later in this file based on what database is used.
          DBT_OUTPUT_SCHEMA: "${{ env.OUTPUT_SCHEMA }}"
          DBT_INPUT_SCHEMA_GITHUB: "${{ inputs.INPUT_SCHEMA_GITHUB }}"
          DBT_INPUT_SCHEMA_FAA: "${{ inputs.INPUT_SCHEMA_FAA }}"
          DBT_INPUT_SCHEMA_EXCHANGERATEHOST: "${{ inputs.INPUT_SCHEMA_EXCHANGERATEHOST }}"
          DBT_INPUT_SCHEMA_ECOMMERCE_DEMO: "${{ inputs.INPUT_SCHEMA_ECOMMERCE_DEMO }}"
          DBT_INPUT_SCHEMA_DATA_SCIENCE: "${{ inputs.INPUT_SCHEMA_DATA_SCIENCE }}"
          DBT_DB_USER: "${{ env.DB_USER }}"
          DBT_DB_WAREHOUSE: "${{ env.DB_WAREHOUSE }}"
          DBT_DB_ACCOUNT: "${{ env.DB_ACCOUNT }}"
          DBT_DB_HOST: "${{ env.DB_HOST }}"
          DBT_DB_PORT: "${{ env.DB_PORT }}"
          DBT_DB_NAME: "${{ env.DB_NAME }}"
          DBT_DB_PASS: "${{ secrets.DB_PASS }}"
          # TODO - move it to separate job dedicated to dbt Cloud
          # Notify by sending comment to the merge request,
          # if duration of a dbt model exceeds average duration from last X runs by DBT_ALLOWED_DEGRADATION percents
          DBT_ALLOWED_DEGRADATION: 20
          DBT_INCREMENTAL_STRATEGY: "merge"
        run: |
          dbt run --profiles-dir $DBT_PROFILES_DIR --profile $ELT_ENVIRONMENT --target $DBT_TARGET $FR_ARG
          dbt test --profiles-dir $DBT_PROFILES_DIR --profile $ELT_ENVIRONMENT --target $DBT_TARGET
          gooddata-dbt provision_workspaces
          gooddata-dbt register_data_sources $GOODDATA_UPPER_CASE --profile $ELT_ENVIRONMENT --target $DBT_TARGET
          gooddata-dbt deploy_ldm $GOODDATA_UPPER_CASE --profile $ELT_ENVIRONMENT --target $DBT_TARGET
          # Invalidates GoodData caches
          gooddata-dbt upload_notification --profile $ELT_ENVIRONMENT --target $DBT_TARGET
