###################
# Templates
.dbt:
  extends:
    - .base
  # We build a custom image on top of the official dbt image in this pipeline
  # It contains dbt itself, all required dbt plugins and gooddata-dbt plugin
  image:
    name: "$DBT_CUSTOM_IMAGE"
    entrypoint: [""]
  stage: transform
  before_script:
    - cd $SRC_DATA_PIPELINE
    # dbt packages are installed during build of docker image to workdir
    - ln -s ${IMAGES_WORKDIR}/dbt_packages dbt_packages
    - if [ "$FULL_REFRESH" == "true" ]; then export FR_ARG="--full-refresh"; else export FR_ARG=""; fi
  script:
    - dbt run --profiles-dir $DBT_PROFILES_DIR --profile $ELT_ENVIRONMENT --target $DBT_TARGET $FR_ARG
    - dbt test --profiles-dir $DBT_PROFILES_DIR --profile $ELT_ENVIRONMENT --target $DBT_TARGET
    - gooddata-dbt deploy_models $GOODDATA_UPPER_CASE --profile $ELT_ENVIRONMENT --target $DBT_TARGET
    # Invalidates GoodData caches
    - gooddata-dbt upload_notification --profile $ELT_ENVIRONMENT --target $DBT_TARGET

.dbt_cloud:
  extends:
    - .base
  # We build a custom image on top of the official dbt image in this pipeline
  # It contains dbt itself, all required dbt plugins and gooddata-dbt plugin
  image:
    name: "$DBT_CUSTOM_IMAGE"
    entrypoint: [""]
  stage: transform
  before_script:
    - cd $SRC_DATA_PIPELINE
  script:
    # Run corresponding job in dbt cloud
    - gooddata-dbt dbt_cloud_run $GOODDATA_UPPER_CASE --profile $ELT_ENVIRONMENT --target $DBT_TARGET
    # Collect dbt metadata from the cloud, generate and deploy corresponding GoodData logical data model
    - gooddata-dbt deploy_models $GOODDATA_UPPER_CASE --profile $ELT_ENVIRONMENT --target $DBT_TARGET
    # Invalidate GoodData caches
    - gooddata-dbt upload_notification --profile $ELT_ENVIRONMENT --target $DBT_TARGET

.dbt_changes:
  changes:
    - $SRC_DATA_PIPELINE/macros/**/*
    - $SRC_DATA_PIPELINE/models/**/*
    - $SRC_DATA_PIPELINE/profile/**/*
    - $SRC_DATA_PIPELINE/dbt_project.yml
    - $SRC_DATA_PIPELINE/packages.yml
    - $SRC_DATA_PIPELINE/requirements-dbt.txt
    - $SRC_DATA_PIPELINE/requirements-gooddata.txt
    - $SRC_DATA_PIPELINE/meltano.yml
    - .gitlab-ci/gitlab-ci-transform.yml
    - .gitlab-ci.yml

variables:
  # Variables for dbt following dbt Cloud convention (DBT_)
  DBT_INPUT_SCHEMA: $INPUT_SCHEMA
  DBT_OUTPUT_SCHEMA: $OUTPUT_SCHEMA
  DBT_INPUT_SCHEMA_GITHUB: $INPUT_SCHEMA_GITHUB
  DBT_INPUT_SCHEMA_FAA: $INPUT_SCHEMA_FAA
  DBT_INPUT_SCHEMA_EXCHANGERATEHOST: $INPUT_SCHEMA_EXCHANGERATEHOST

  DBT_SNOWFLAKE_ACCOUNT: $SNOWFLAKE_ACCOUNT
  DBT_SNOWFLAKE_USER: $SNOWFLAKE_USER
  DBT_SNOWFLAKE_PASS: $SNOWFLAKE_PASS
  DBT_SNOWFLAKE_DBNAME: $SNOWFLAKE_DBNAME
  DBT_SNOWFLAKE_WAREHOUSE: $SNOWFLAKE_WAREHOUSE

  DBT_VERTICA_HOST: $VERTICA_HOST
  DBT_VERTICA_PORT: $VERTICA_PORT
  DBT_VERTICA_USER: $VERTICA_USER
  DBT_VERTICA_PASS: $VERTICA_PASS
  DBT_VERTICA_DBNAME: $VERTICA_DBNAME
  # Notify by sending comment to the merge request,
  # if duration of a dbt model exceeds average duration from last X runs by DBT_ALLOWED_DEGRADATION percents
  DBT_ALLOWED_DEGRADATION: 20

##########################
# Jobs

# pre-merge
dbt_dev:
  extends:
    - .dbt
  variables:
    ELT_ENVIRONMENT: $DEV_ELT_ENVIRONMENT
    DBT_SNOWFLAKE_DBNAME: $DEV_SNOWFLAKE_DBNAME
    GOODDATA_ENVIRONMENT_ID: $DEV_GOODDATA_ENVIRONMENT_ID
  rules:
    - if: '$CI_MERGE_REQUEST_TARGET_BRANCH_NAME == "main"'
      changes: !reference [.dbt_changes, changes]
    - !reference [.base_rules, rules]
    - !reference [.elt_rules, rules]

# pre-merge dbt Cloud
dbt_cloud_dev:
  extends:
    - .dbt_cloud
  variables:
    DBT_JOB_ID: $CLOUD_DEV_DBT_JOB_ID
    ELT_ENVIRONMENT: $DEV_ELT_ENVIRONMENT
    DBT_SNOWFLAKE_DBNAME: $CLOUD_DEV_SNOWFLAKE_DBNAME
    GOODDATA_ENVIRONMENT_ID: $CLOUD_DEV_GOODDATA_ENVIRONMENT_ID
  rules:
    - if: '$CI_MERGE_REQUEST_TARGET_BRANCH_NAME == "main"'
      changes: !reference [.dbt_changes, changes]
    - !reference [.base_rules, rules]
    - !reference [.elt_rules, rules]

# post-merge
dbt_staging:
  extends:
    - .dbt
  variables:
    ELT_ENVIRONMENT: $STAGING_ELT_ENVIRONMENT
    DBT_SNOWFLAKE_DBNAME: $STAGING_SNOWFLAKE_DBNAME
    GOODDATA_ENVIRONMENT_ID: $STAGING_GOODDATA_ENVIRONMENT_ID
  rules:
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
      changes: !reference [.dbt_changes, changes]
    - if: '$CI_PIPELINE_SOURCE == "schedule" && $RUN_ETL_STAGING == "true"'
    - !reference [.base_rules, rules]
    - !reference [.elt_rules, rules]

# post-merge dbt Cloud
dbt_cloud_staging:
  extends:
    - .dbt_cloud
  variables:
    DBT_JOB_ID: $CLOUD_STAGING_DBT_JOB_ID
    ELT_ENVIRONMENT: $STAGING_ELT_ENVIRONMENT
    DBT_SNOWFLAKE_DBNAME: $CLOUD_STAGING_SNOWFLAKE_DBNAME
    GOODDATA_ENVIRONMENT_ID: $CLOUD_STAGING_GOODDATA_ENVIRONMENT_ID
  rules:
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
      changes: !reference [.dbt_changes, changes]
    - if: '$CI_PIPELINE_SOURCE == "schedule" && $RUN_ETL_STAGING == "true"'
    - !reference [.base_rules, rules]
    - !reference [.elt_rules, rules]

dbt_staging_vertica:
  extends:
    - .dbt
  variables:
    ELT_ENVIRONMENT: $STAGING_ELT_ENVIRONMENT
    GOODDATA_ENVIRONMENT_ID: $STAGING_GOODDATA_ENVIRONMENT_ID_VERTICA
    DBT_TARGET: "vertica"
    GOODDATA_UPPER_CASE: ""
  rules:
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"'
      changes: !reference [.dbt_changes, changes]
    - if: '$CI_PIPELINE_SOURCE == "schedule" && $RUN_ETL_STAGING == "true"'
    - !reference [.base_rules, rules]
    - !reference [.elt_rules, rules]

# merge to prod branch
dbt_prod:
  extends:
    - .dbt
  variables:
    ELT_ENVIRONMENT: $PROD_ELT_ENVIRONMENT
    DBT_SNOWFLAKE_DBNAME: $PROD_SNOWFLAKE_DBNAME
    GOODDATA_ENVIRONMENT_ID: $PROD_GOODDATA_ENVIRONMENT_ID
  rules:
    - if: '$CI_COMMIT_BRANCH == "prod" && $CI_PIPELINE_SOURCE == "push"'
      changes: !reference [.dbt_changes, changes]
    # The pipeline scheduler triggers only PROD jobs
    - if: '$CI_PIPELINE_SOURCE == "schedule" && $RUN_ETL_PROD == "true"'
    - !reference [.base_rules, rules]
    - !reference [.elt_rules, rules]

# merge to prod dbt Cloud
dbt_cloud_prod:
  extends:
    - .dbt_cloud
  variables:
    DBT_JOB_ID: $CLOUD_PROD_DBT_JOB_ID
    ELT_ENVIRONMENT: $PROD_ELT_ENVIRONMENT
    DBT_SNOWFLAKE_DBNAME: $CLOUD_PROD_SNOWFLAKE_DBNAME
    GOODDATA_ENVIRONMENT_ID: $CLOUD_PROD_GOODDATA_ENVIRONMENT_ID
  rules:
    - if: '$CI_COMMIT_BRANCH == "prod" && $CI_PIPELINE_SOURCE == "push"'
      changes: !reference [.dbt_changes, changes]
    # The pipeline scheduler triggers only PROD jobs
    - if: '$CI_PIPELINE_SOURCE == "schedule" && $RUN_ETL_PROD == "true"'
    - !reference [.base_rules, rules]
    - !reference [.elt_rules, rules]
